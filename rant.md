# PyTorch3D's journey through the NeurIPS review process

We recently submitted PyTorch3D to NeurIPS 2020 and I thought I should share some thoughts from our experience submitting a paper of that nature to a ML/AI conference. This piece is not meant to attack the AC and the reviewers, but rather shed some light on the things that are going terribly wrong in our community.

## Background

PyTorch3D is by far the most challenging project I have ever worked on, both as a contributor (doing core engineering work, aka writing CUDA and C++) and as a lead (shaping the project). PyTorch3D has been one of those projects that helped me grow, and still does. I came out of it a better scientist, a better engineer, a better conceptualizer, a better communicator and a better team player. That doesn't happen often. For how many projects in your career can you say that? I bet not many! The challenges behind PyTorch3D do not just involve developping a detailed understanding of geometry and data representations, or figuring out how to turn complex math into code, but to also design and engineer geometric operators that are efficient, modular, differentiabile and learning friendly. Getting these right makes implementation non trivial. To put it into perspective, no other 3D deep learning library as of today allows batches of 3D data as inputs to their operators. As a contrast, batching is present in all deep learning frameworks like PyTorch or Tensorflow and lack of it will render any such framework non usable immediately! 

We knew that our submission to NeurIPS 2020 would be a new experience for us. None of the authors had submitted a paper like this before. Our reviews were disheartening, yet predictable. Please note that my goal is not to shame reviewers. Reviewers provide their service voluntarily and I deeply thank them for their time and energy! But the review process revelead to me that our community overall is short-sighted and unaccepting of anything out of the norm. 

The first comment shared by all reviewers was that our submission lacked "novelty". This word, seen so often in reviews, triggers so many emotions in me! It is abused, it lacks content and has no place in a scientific/academic review process. 

##  What is "novelty"?

I made the decision to spend a paragraph from the 1 page rebuttal to address the comment that PyTorch3D is not "novel". I am still not sure it was the smartest move, but I felt I had to do set an example for our more junior, yet brilliant, collaborators who felt disheartened by this shallow comment. Our response follows 

> Reviewers seem to believe that our work lacks *novelty and interesting research insights*. We disagree and restate our novelty in the next paragraph. However, *is algorithmic novelty the sole factor for a community to move forward?* Consider the progress in deep learning libraries and their immense impact in the ML/AI community. Back in 2013, the predominant libraries were LuaTorch & Caffe.
While groundbreaking, they were difficult to use and not modular enough for research; users were required to change or implement new CUDA kernels to customize their computation. A few years later, TF & PyTorch offered the exact same computational ability as LuaTorch and Caffe but innovated in two axes: (a) modularity and (b) efficiency. Users can now customize their computation easily without writing CUDA while maintaining or even improving efficiency for batched data. PyTorch & TF are not algorithmically novel nor do they offer new research insights, but their impact through careful and creative engineering has led to unprecedented growth in ML/AI. So, are the contributions of PyTorch or TF not worthy to be presented at a conference like NeurIPS? We strongly believe they are! And the community agrees -- PyTorch was presented in NeurIPS 2019. PyTorch3D follows the same design principles of efficiency and modularity for the case of 3D.
R1 believes that our contribution is *less creative*. We disagree! Similar to novelty, *creativity* takes many forms.
We had to be creative in the way we redesigned and reimplemented 3D operators to handle heterogeneous batches of 3D data while maintaining or even improving computational efficiency. Undoubtedly, much of our creativity lives in our design and implementations rather than algorithmic changes. But it is creativity nevertheless! While we are happy that all reviewers acknowledge the merits of our work and its impact on the community, we are saddened to see that the concepts of *novelty* and *creativity* are interpreted short-sightedly. PyTorch3D is novel and creative and that is how it achieves modularity and efficiency.

## How I address "novelty" as an AC

I strongly believe that the word "novelty" should be banned in reviews. No work is novel, and if you believe that then you are hallucinating! We draw inspiration and build on past work in order to move forward. Now, reviewers are usually PhD students who believe that every submission is a threat to their work. I was a reviewer while a PhD student and I know the feeling! It takes a certain academic and scientific maturity to snap out of that feeling and realize that growth is part of our world. A new submission that is building, being inspired and improving your work is not a threat but a compliment! Here is where the role of the PhD advisor and the AC comes in. 

As an AC, I read the reviews before they are sent out to authors. I particularly look for the use of the word "novelty" and ask reviewers to replace it with indisputable evidence that shows that (a) the submission is closely connected to other peer-reviewed references, (b) these references clearly and univocally state the contributions potentially shared with the submission (I ask for paper, page and line where the statement is made) and (c) the authors are be expected to be aware of the contributions made by these references. If reviewers cannot provide proof for (a) or (b) or (c) then reviewers have no grounds to make any claims. 

I adopted the aforementioned reaction to the term "novelty" thanks to an AC when I was a reviewer for CVPR years ago and when I was abusing the "novelty" term myself in my reviews. This AC emailed me and asked to me to provide factual arguments to back up my claims. They were pretty adamant about it. I was embarassed but they were right and whoever that AC was I deeply thank them for calling me out! That was a pivotal moment for me that made me realize that I was an immature member of the program committee! I have since never used the novelty term frivolously. 

## The AC's response to our rebuttal

The rest of our rebuttal addressed the reviewers' oversight regarding our work's contributions, clarifications on things they did not understand and emphasize the stuff they seemed to have completely missed. Typical stuff! I am not here to rant about the quality of the reviews. That's a battle for another time!

The response from the AC in their meta-review, who acknowledged the contribution of our work and we thank them for that, was disheartening, even more so than the reviews. The AC response was that our submission is similar to another work and referenced a url of a single line in a github repository. The line of code which was `var1, var2, \` (I kid you not!) is obviously uninformative - you don't need to be a genius programmer to realize that. We searched and searched to find a description in the codebase and the corresponding paper hoping that there is a more enlighting explanation of the line or the algorithm presented in the paper that has any, even a slight, similarity to PyTorch3D but could not find anything. So all we know is that the AC and the reviewers think that PyTorch3D and its many technical contributions and functionality explained in 8 pages in our submission and thousands of lines of code in our codebase are similar to this one line of code in another repository which is not explained anywhere and is relevant to one out of the dozens of operators we support and cover in our submission! Again, I don't mean to shame the AC but I think that if this is the best the senior and reliable members of our community can do, then we have A LOT OF WORK ahead of us to build a scientifically inclusive community.

If we can't be accepting of scientific diversity in contributions in 2020, how on earth will we overcome racial and gender discrimination in AI/ML? I am trying to be hopeful! We have a lot of work to do.
