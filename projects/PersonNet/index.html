<html>
<head>
<style type="text/css">
/* Stolen from Sergey stolen from Jon Barron */
body
{
background-color:#FFFFFF;
color:#222222;
}
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 14px
  }
  strong {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 13px
  }
  heading {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 15px;
        font-weight: 700
  }

  h5 {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700;
    color: orange;
  }
  </style>

<title>R-CNNs for Pose Estimation and Action Detection</title>

</head>

<body>
<h2>R-CNNs for Pose Estimation and Action Detection</h2>
<h4>March, 2015</h4>

<p style="text-align: center;">
<img height=266 src="multiloss.png" />
</p>

<h4>Abstract</h4>
<p>
We present convolutional neural networks for the tasks of keypoint (pose) prediction
and action classification of people in unconstrained images. Our approach
involves training an R-CNN detector with loss functions depending on the task
being tackled. We evaluate our method on the challenging PASCAL VOC dataset
and compare it to previous leading approaches. Our method gives state-of-theart
results for keypoint and action prediction. Additionally, we introduce a new
dataset for action detection, the task of simultaneously localizing people and classifying
their actions, and present results using our approach.
<br /><br />
<a href="https://www.dropbox.com/s/nv2o0hpdt9n8fak/multiloss.pdf?dl=0">paper</a>
</p>

<hr />

<h4>Deploy Code</h4>
<p>
Before using the available source code, you need to install <a href="http://caffe.berkeleyvision.org/">Caffe</a>. </br>

<h5>Action Classification</h5> 

Download: <a href="https://www.dropbox.com/s/2xdkfeahsdj1h03/Action.tar.gz?dl=0">Action.tar.gz</a> </br>
<ul>
  <li><font color="009999">pascal_finetune_fc7.prototxt</font> prototxt to extract fc7 features</li>
  <li><font color="009999">pascal_finetune_HumanNet2_iter_10000</font> model weights trained on PASCAL VOC 2012 Action train set</li>
</ul>


<h5>Keypoint Prediction</h5> 

Download: <a href="https://www.dropbox.com/s/fowykgerfwmrgl5/Pose.tar.gz?dl=0">Pose.tar.gz</a> </br>
<ul>
  <li><font color="009999">kp_reg_deploy.prototxt</font> prototxt to extract fc8 features</li>
  <li><font color="009999">pascal_finetune_HumanNet3_flip_iter_90000</font> model weights trained on PASCAL VOC 2012 Main train set</li>
  <li><font color="009999">fc8_to_coords.m</font> transforms fc8 features to keypoint coordinates (in matlab)</li>
  <li><font color="009999">kps_names.txt</font> list of names of the keypoints</li>
</ul>

<h5>Person Detection+Action+Pose</h5> 

Download: <a href="https://www.dropbox.com/s/z60u5ozhqw02e15/All.tar.gz?dl=0">All.tar.gz</a> </br>
<ul>
  <li><font color="009999">pascal_finetune_fc7.prototxt</font> prototxt to extract fc7 features</li>
  <li><font color="009999">pascal_finetune_HumanNet123_iter_100000</font> model weights trained on PASCAL VOC 2012 Main+Action train set jointly for the three tasks</li>
</ul>

<br/>

</p>


<hr />

<h4>Dataset</h4>
<p>
You can download the action dataset, as used in the paper. The dataset contains the PASCAL VOC Action 2012 images, with complete annotations of all the people and their action labels.

<br/>
<br/>
Dataset download:  <a href="https://www.dropbox.com/s/va4jv2t8kxan7wx/action_dataset.tar.gz?dl=0">action_dataset.tar.gz</a>
</p>

<hr />

<h4>How to cite</h4>

<p>
When citing our system, please cite this work. The bibtex entry is provided below
for your convenience.
</p>

<p>
<tt>
 @inproceedings{poseactionrcnn,<br />
  &nbsp;Author = {G. Gkioxari and B. Hariharan and R. Girshick and J. Malik},<br />
  &nbsp;Title = {R-CNNs for Pose Estimation and Action Detection},<br />
  &nbsp;ArchivePrefix = {arXiv},<br />
  &nbsp;Eprint = {1406.5212},<br />
  &nbsp;PrimaryClass = {cs.CV},<br />
  &nbsp;Year = {2014}}
</tt>
</p>

<hr />

<h4>Contact</h4>
<p>
For any questions regarding the work or the implementation, contact the author at gkioxari@eecs.berkeley.edu 
</p>

</body>

</html>
